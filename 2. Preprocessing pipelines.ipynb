{"cells":[{"cell_type":"markdown","source":["#MA707 Report - Preprocessing (spring 2019, DataHeroes)"],"metadata":{}},{"cell_type":"markdown","source":["## Introduction\n\nThis notebook contains code to create feature-target datasets. Here the two merged dataframes `bci_coal_pdf` and `bci_ironore_pdf` will go through pre-processing using the `FeatureUnionDF` class which conatins several `Feature Selection` and `Feature Creation` classes to provide a clean and complete dataset which will be used to train and test and investigate the regression models later in the report. Inorder to provide a better prediction of the response variable, all the feature variables are lagged with certain numbers.\n\nThese datasets will be split into training and test datasets which will then be used in cross-validation of the models."],"metadata":{}},{"cell_type":"markdown","source":["## Contents\n1. Setup\n2. Pipeline creation\n2. Create train and test datasets\n3. Summary"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Setup"],"metadata":{}},{"cell_type":"code","source":["%run \"./1. Class demonstrations\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def display_pdf(a_pdf):\n  display(spark.createDataFrame(a_pdf))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["def est_grid_results_pdf(my_est_grid_obj,est_tag=None,fea_tag=None): \n  import pandas as pd\n  import numpy  as np\n  res_pdf = pd.DataFrame(data=my_est_grid_obj.cv_results_) \\\n           .loc[:,lambda df: np.logical_or(df.columns.str.startswith('param_'),\n                                           df.columns.str.endswith('test_score'))\n               ] \\\n           .loc[:,lambda df: np.logical_not(df.columns.str.startswith('split'))\n               ] \\\n           .drop(['std_test_score'], \n                 axis=1)\n  res_pdf.columns = [column.replace('param_','') for column in list(res_pdf.columns)]\n  if est_tag is not None: res_pdf = res_pdf.assign(est_tag=est_tag)\n  if fea_tag is not None: res_pdf = res_pdf.assign(fea_tag=fea_tag)\n  return res_pdf.sort_values('mean_test_score')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["In the above code `est_grid_results_pdf()` function is created which takes the sklearn GridSearchCV output as input and performs the below operations:\n\n- Using the attribute `cv_results` of the GridSearchCV function which returns all the test scores as a numpy dictionary for any given execution and save the results in a pandas dataframe using `pd.DataFrame()` function.\n- Using the resulting dataframe from previous step it checks all the columns and selects only columns which ends with `_test_score` or columns starting with `param_` to a new dataframe. It then drops the column `std_test_score` from the dataframe and finally returns the gridsearch output as a pandas dataframe using function with all the test score and ranks based on all assigned hyperparameters."],"metadata":{}},{"cell_type":"markdown","source":["### 2. Pipeline Creation"],"metadata":{}},{"cell_type":"markdown","source":["Create a function `get_count_vect_ore_three_plus_all_ts_pipe()` which returns a pipeline which then will be used to return a processed dataframe after performing all the pre-processing transformations for the merged dataframe `bci_ironore_pdf`."],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_count_vect_ore_three_plus_all_ts_pipe():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  return Pipeline(steps=[\n    ('fea_one', FeatureUnionDF(transformer_list=[\n      ('tgt_var'    ,CreateTargetVarDF(var='bci_5tc')),\n      ('dt_vars'    ,CreateDatetimeVarsDF(var='date')),\n      ('lag_ts_vars',CreateLagVarsDF(\n        var_list=['cme_ln2','rici','p1a_03','p4_03','c7',\n                  'cme_ln3','p3a_03','shfe_rb3','shfe_al3',\n                  'shfe_cu3','ice_tib3','cme_fc3','opec_orb',\n                  'ice_sb3','p3a_iv','ice_kc3','c5',\n                  'p2a_03','cme_lc2','cme_sm3','ice_tib4','bci','cme_ln1','cme_s2'],\n        lag_list=[3])),\n      ('lag_txt_vars',CreateLagVarsDF(var_list=['tags_ore','content_ore','title_ore'],\n                                      lag_list=[3])),\n    ])),\n    ('drop_na_rows'  ,DropNaRowsDF(how='any')),\n    ('fea_two', FeatureUnionDF(transformer_list=[\n      ('named_vars' ,CreateNamedVarsDF(except_list=['tags_ore_lag3','content_ore_lag3','title_ore_lag3'])),\n      ('cnt_tags'   , CountVectColDF(col_name=   'tags_ore_lag3',prefix='cnt_ore_tags_'   ,add_stop_words=[])),\n      ('cnt_content', CountVectColDF(col_name='content_ore_lag3',prefix='cnt_ore_content_',add_stop_words=[])),  \n      ('cnt_title'  , CountVectColDF(col_name=  'title_ore_lag3',prefix='cnt_ore_title_'  ,add_stop_words=[])),  \n    ])),\n    ('drop_na_rows_again'  ,DropNaRowsDF(how='any')),\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["`FeatureUnion` combines several transformer objects into a new transformer that combines their output. \n\nAbove function `get_count_vect_ore_three_plus_all_ts_pipe()` code returns a pipeline which have two feature unions `fea_one` and `fea_two`. \n\nThe first feature union `fea_one` dose the below operations.  \n - The object `tgt_var` which uses the `CreateTargetVarDF()` method to create target dataset with column `bci_5tc` using the input dataframe. \n - Then object `dt_vars` which uses the `CreateDatetimeVarsDF()` method it takes `date` columns as input and creates many individual columns of `year`, `month`, `day`,          `hour`, `weekofyear`,`weekday` and `dayofyear`.    \n - Next object `lag_ts_vars` which uses the `CreateLagVarsDF()` method using the input columns names from the `var_list` and create the new time series lag variable list of      all of them with the number of days given in the `lag_list`. \n - Next object `lag_txt_vars` which uses the `CreateLagVarsDF()` method using the input columns names from the `var_list` and create the new text lag variable list of all of    them with the number of days given in the `lag_list`. \n\nThe Output of the first feature union is passed to object `drop_na_rows` which uses the `DropNaRowsDF()` method which drops all the rows which have any missing data. \n\nThe second feature union `fea_two` takes the cleaned dataframe with no missing values created from the output of the method `DropNaRowsDF()` and does the below operations. \n - The object `named_vars` uses the `CreateNamedVarsDF()` method using the columns names from the list `execpt_list` and excludes these columns from the original dataframe column list. \n - Then object `cnt_tags` which uses the `CountVectColDF()` method creates new features also known as tokens for each individual document and prints the counts of each tokens in every row in the defined variable column into an array with the columns names prefixing with `cnt_tags_`. It also removes the tokens/variables for the words mentioned in the list `add_stop_words`. As a default list it takes the `sklearn stop word` list and it doesn't create new variables for them. The feature union then concatenates these newly created features into the dataframe.\n - The above steps are performed for all the three columns `tags_ore_lag3`, `content_ore_lag3` and `title_ore_lag3` and similarly all the newly created tokens/variables are concatenated to the existing dataframe which then returns the existing features with all the tokens and the count for each observations. \n \nThe Output of the second feature union is passed to object `drop_na_rows` which uses the `DropNaRowsDF()` method which drops all rows with any missing data."],"metadata":{}},{"cell_type":"markdown","source":["Create another function `get_tfidf_vect_ore_three_plus_all_ts_pipe()` to perform the same operation on the merged dataframe `bci_ironore_pdf` but using the `TfidfVectColDF` instead of the `CountVectColDF` which produces values of the product of the **term-frequency and inverse document frequency** for the newly created tokenized variables and concatenated to the dataframe with the lagged variables."],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_tfidf_vect_ore_three_plus_all_ts_pipe():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  return Pipeline(steps=[\n    ('fea_one', FeatureUnionDF(transformer_list=[\n      ('tgt_var'    ,CreateTargetVarDF(var='bci_5tc')),\n      ('dt_vars'    ,CreateDatetimeVarsDF(var='date')),\n      ('lag_ts_vars',CreateLagVarsDF(\n        var_list=['cme_ln2','rici','p1a_03','p4_03','c7',\n                  'cme_ln3','p3a_03','shfe_rb3','shfe_al3',\n                  'shfe_cu3','ice_tib3','cme_fc3','opec_orb',\n                  'ice_sb3','p3a_iv','ice_kc3','c5',\n                  'p2a_03','cme_lc2','cme_sm3','ice_tib4','bci','cme_ln1','cme_s2'],\n        lag_list=[3])),\n      ('lag_txt_vars',CreateLagVarsDF(var_list=['tags_ore','content_ore','title_ore'],\n                                      lag_list=[3])),\n    ])),\n    ('drop_na_rows'  ,DropNaRowsDF(how='any')),\n    ('fea_two', FeatureUnionDF(transformer_list=[\n      ('named_vars' ,CreateNamedVarsDF(except_list=['tags_ore_lag3','content_ore_lag3','title_ore_lag3'])),\n      ('tfidf_tags'   , TfidfVectColDF(col_name=   'tags_ore_lag3',prefix='tfidf_tags_'   ,add_stop_words=[])),\n      ('tfidf_content', TfidfVectColDF(col_name='content_ore_lag3',prefix='tfidf_content_',add_stop_words=[])),  \n      ('tfidf_title'  , TfidfVectColDF(col_name=  'title_ore_lag3',prefix='tfidf_title_'  ,add_stop_words=[])),    \n    ])),\n    ('drop_na_rows_again'  ,DropNaRowsDF(how='any')),\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["The function `get_tfidf_vect_coal_three_plus_all_ts_pipe()` below returns a pipeline to pre-process the merged dataframe `bci_coal_pdf` and return the pre-processed dataframe with all the existing features as well as the newly created variables created from the tokenization of the `tag_coal`, `content_coal` and `title_coal` columns with their respective `tfidf values`"],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_tfidf_vect_coal_three_plus_all_ts_pipe():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  return Pipeline(steps=[\n    ('fea_one', FeatureUnionDF(transformer_list=[\n      ('tgt_var'    ,CreateTargetVarDF(var='bci_5tc')),\n      ('dt_vars'    ,CreateDatetimeVarsDF(var='date')),\n      ('lag_ts_vars',CreateLagVarsDF(\n        var_list=['cme_ln2','rici','p1a_03','p4_03','c7',\n                  'cme_ln3','p3a_03','shfe_rb3','shfe_al3',\n                  'shfe_cu3','ice_tib3','cme_fc3','opec_orb',\n                  'ice_sb3','p3a_iv','ice_kc3','c5',\n                  'p2a_03','cme_lc2','cme_sm3','ice_tib4','bci','cme_ln1','cme_s2'],\n        lag_list=[3])),\n      ('lag_txt_vars',CreateLagVarsDF(var_list=['tags_coal','content_coal','title_coal'],\n                                      lag_list=[3])),\n    ])),\n    ('drop_na_rows'  ,DropNaRowsDF(how='any')),\n    ('fea_two', FeatureUnionDF(transformer_list=[\n      ('named_vars' ,CreateNamedVarsDF(except_list=['tags_coal_lag3','content_coal_lag3','title_coal_lag3'])),\n      ('tfidf_tags'   , TfidfVectColDF(col_name=   'tags_coal_lag3',prefix='tfidf_tags_'   ,add_stop_words=[])),\n      ('tfidf_content', TfidfVectColDF(col_name='content_coal_lag3',prefix='tfidf_content_',add_stop_words=[])),  \n      ('tfidf_title'  , TfidfVectColDF(col_name=  'title_coal_lag3',prefix='tfidf_title_'  ,add_stop_words=[])),    \n    ])),\n    ('drop_na_rows_again'  ,DropNaRowsDF(how='any')),\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["The above function `get_tfidf_vect_coal_three_plus_all_ts_pipe()` also returns a pipeline which have two feature unions `fea_one` and `fea_two` which then will return a dataframe with all the lagged version of the variables in the `bci_pdf` and the new features created by tokenizing the `tag`, `content`, `title` columns concatenated to it with all `tfidf` values as their values. It also contains features created from the `datetime` variable as the `year`, `month`, `day`, `hour`, `weekofyear`,`weekday` and `dayofyear`.\n\nIt will return the cleaned pre-processed dataset by removing all missing values if we fit the merged dataframe `bci_coal_pdf`."],"metadata":{}},{"cell_type":"markdown","source":["Similarly the below function creates a pipeline to transform the `bci_coal_pdf` dataframe with lagged variables alongwith the new tokeneized variables and their frequency count. It then removes all rows with missing values using the `drop_na_rows_again` object which drops all rows with any missing value set by the parameter in the method `DropNaRowsDF(how='any')`."],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_count_vect_coal_three_plus_all_ts_pipe():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  return Pipeline(steps=[\n    ('fea_one', FeatureUnionDF(transformer_list=[\n      ('tgt_var'    ,CreateTargetVarDF(var='bci_5tc')),\n      ('dt_vars'    ,CreateDatetimeVarsDF(var='date')),\n      ('lag_ts_vars',CreateLagVarsDF(\n        var_list=['cme_ln2','rici','p1a_03','p4_03','c7',\n                  'cme_ln3','p3a_03','shfe_rb3','shfe_al3',\n                  'shfe_cu3','ice_tib3','cme_fc3','opec_orb',\n                  'ice_sb3','p3a_iv','ice_kc3','c5',\n                  'p2a_03','cme_lc2','cme_sm3','ice_tib4','bci','cme_ln1','cme_s2'],\n        lag_list=[3])),\n      ('lag_txt_vars',CreateLagVarsDF(var_list=['tags_coal','content_coal','title_coal'],\n                                      lag_list=[3])),\n    ])),\n    ('drop_na_rows'  ,DropNaRowsDF(how='any')),\n    ('fea_two', FeatureUnionDF(transformer_list=[\n      ('named_vars' ,CreateNamedVarsDF(except_list=['tags_coal_lag3','content_coal_lag3','title_coal_lag3'])),\n      ('cnt_tags'   , CountVectColDF(col_name=   'tags_coal_lag3',prefix='cnt_coal_tags_'   ,add_stop_words=[])),\n      ('cnt_content', CountVectColDF(col_name='content_coal_lag3',prefix='cnt_coal_content_',add_stop_words=[])),  \n      ('cnt_title'  , CountVectColDF(col_name=  'title_coal_lag3',prefix='cnt_coal_title_'  ,add_stop_words=[])),  \n    ])),\n    ('drop_na_rows_again'  ,DropNaRowsDF(how='any')),\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["### Fitting datasets into the pipeline functions"],"metadata":{}},{"cell_type":"markdown","source":["#### Dataframe 1: `fea_tgt_coal_tfidf_pdf`\nFit the merged dataframe `bci_coal_pdf` into the above defined function `get_tfidf_vect_coal_three_plus_all_ts_pipe()` and create a new dataframe `fea_tgt_coal_tfidf_pdf` which cotains the lagged time series feature values and the features created by `TfidfVectorizer` method with the `tf-idf` values of the extracted variables from the text columns in the dataframe, and all the NA rows dropped."],"metadata":{}},{"cell_type":"code","source":["fea_tgt_coal_tfidf_pdf = \\\n  get_tfidf_vect_coal_three_plus_all_ts_pipe() \\\n  .fit(bci_coal_pdf) \\\n  .transform(bci_coal_pdf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["fea_tgt_coal_tfidf_pdf.info()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;\nInt64Index: 1591 entries, 3 to 1593\nColumns: 41766 entries, cme_s2_lag3 to tfidf_title_zone\ndtypes: float64(41766)\nmemory usage: 507.0 MB\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["The new processed dataframe `fea_tgt_coal_tfidf_pdf` contains 1591 rows with 41,766 variables in it. This dataframe will be used later to train and test different regression models to get the best fitted model to predict the `bci_5tc` value."],"metadata":{}},{"cell_type":"markdown","source":["#### Dataframe 2: `fea_tgt_coal_cnt_pdf`\nSimilarly, the `bci_coal_pdf` dataframe is fitted into the second function `get_count_vect_coal_three_plus_all_ts_pipe()` which then returns the dataframe `fea_tgt_coal_cnt_pdf` containing the lagged time series features as well the tokenized features with their count frequency in each document or observations from the text columns in the dataframe."],"metadata":{}},{"cell_type":"code","source":["fea_tgt_coal_cnt_pdf = \\\n  get_count_vect_coal_three_plus_all_ts_pipe() \\\n  .fit(bci_coal_pdf) \\\n  .transform(bci_coal_pdf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["fea_tgt_coal_cnt_pdf.info()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;\nInt64Index: 1591 entries, 3 to 1593\nColumns: 41766 entries, cme_s2_lag3 to cnt_coal_title_zone\ndtypes: float64(41766)\nmemory usage: 507.0 MB\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["#### Dataframe 3: `fea_tgt_ore_tfidf_pdf`\nPerfom the same `fit` and `transform` operations on the `bci_iron_ore` dataframe and create two new dataframes with one having the count of tokens in each documents concatenated as features to the lagged time series features while the other the product `tf-idf` value for each tokens in every documents in the text columns of the dataframe."],"metadata":{}},{"cell_type":"code","source":["fea_tgt_ore_tfidf_pdf = \\\n  get_tfidf_vect_ore_three_plus_all_ts_pipe() \\\n  .fit(bci_ironore_pdf) \\\n  .transform(bci_ironore_pdf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["fea_tgt_ore_tfidf_pdf.info()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;\nInt64Index: 1592 entries, 3 to 1594\nColumns: 39617 entries, cme_s2_lag3 to tfidf_title_zuckerberg\ndtypes: float64(39617)\nmemory usage: 481.2 MB\n</div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["#### Dataframe 4: `fea_tgt_ore_cnt_pdf`"],"metadata":{}},{"cell_type":"code","source":["fea_tgt_ore_cnt_pdf = \\\n  get_count_vect_ore_three_plus_all_ts_pipe() \\\n  .fit(bci_ironore_pdf) \\\n  .transform(bci_ironore_pdf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["fea_tgt_ore_cnt_pdf.info()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;\nInt64Index: 1592 entries, 3 to 1594\nColumns: 39617 entries, cme_s2_lag3 to cnt_ore_title_zuckerberg\ndtypes: float64(39617)\nmemory usage: 481.2 MB\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["The two new dataframes `fea_tgt_ore_tfidf_pdf` and `fea_tgt_ore_cnt_pdf` contains 1592 rows of observations with 39617 variables. These two will be used to train and test all models and select the best regression model."],"metadata":{}},{"cell_type":"markdown","source":["## 3. Create train and test datasets"],"metadata":{}},{"cell_type":"code","source":["def create_train_test_ts(fea_pdf, tgt_ser, trn_prop=0.8):\n  trn_len = int(trn_prop * len(fea_pdf))\n  return (fea_pdf.iloc[:trn_len],\n          fea_pdf.iloc[ trn_len:],\n          tgt_ser.iloc[:trn_len],\n          tgt_ser.iloc[ trn_len:]\n         )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["The above function divides feature dataset and target dataset into training set and test set. The training percentage is defined as `trn_prop=0.8` which means 80% of the dataset will be used for training the model and the rest for testing the model. \nThe traning row length `trn_len`is the integer of the total row number multiply training percentage. Then the training set subtract `trn_len` rows \nof the original feature and target datasets using `iloc` function."],"metadata":{}},{"cell_type":"markdown","source":["Call the `create_train_test_ts()` function, fit the dataframe `fea_tgt_coal_tdif_pdf` with the `target` variable dropped as feature dataframe, and extract the varibale `target` from `fea_tgt_coal_tdif_pdf` as target dataset. Use function to split training set and test set on the feature dataset and target dataset.\n\nPrint the shape of the training and testing datasets created."],"metadata":{}},{"cell_type":"code","source":["(trn_coal_tfidf_fea_pdf, tst_coal_tfidf_fea_pdf, \n trn_coal_tfidf_tgt_ser, tst_coal_tfidf_tgt_ser\n) = \\\ncreate_train_test_ts(fea_pdf = fea_tgt_coal_tfidf_pdf.drop( 'target',axis=1),\n                     tgt_ser = fea_tgt_coal_tfidf_pdf.loc[:,'target'],\n                    )\n\ntrn_coal_tfidf_fea_pdf.shape, tst_coal_tfidf_fea_pdf.shape, trn_coal_tfidf_tgt_ser.shape, tst_coal_tfidf_tgt_ser.shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">75</span><span class=\"ansired\">]: </span>((1272, 41765), (319, 41765), (1272,), (319,))\n</div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["From the 1591 rows with 41765 variables, the training dataset will have 1272 observations with 41765 columns and the rest 319 rows as the testing dataset. As the target variable is a single dimensional variables, it is equally split into 1272 and 319 rows as training and testing sets respectively."],"metadata":{}},{"cell_type":"markdown","source":["Similar to the above train-test split of the `fea_tgt_coal_tfidf_pdf`, all the other three dataframes created from the featureunion are split using the `create_train_test_ts` function with a training percentage of 80%."],"metadata":{}},{"cell_type":"code","source":["(trn_coal_cnt_fea_pdf, tst_coal_cnt_fea_pdf, \n trn_coal_cnt_tgt_ser, tst_coal_cnt_tgt_ser\n) = \\\ncreate_train_test_ts(fea_pdf = fea_tgt_coal_cnt_pdf.drop( 'target',axis=1),\n                     tgt_ser = fea_tgt_coal_cnt_pdf.loc[:,'target'],\n                    )\n\ntrn_coal_cnt_fea_pdf.shape, tst_coal_cnt_fea_pdf.shape, trn_coal_cnt_tgt_ser.shape, tst_coal_cnt_tgt_ser.shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">76</span><span class=\"ansired\">]: </span>((1272, 41765), (319, 41765), (1272,), (319,))\n</div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["Similarly for the dataframe `fea_tgt_ore_tfidf_pdf` which has 1592 rows and 39616 variables, the train test split created 1273 observations for training the model and 319 observations to test the model for both the feature and target. The shape of the training and test feature and target dataset are printed accordingly."],"metadata":{}},{"cell_type":"code","source":["(trn_ore_tfidf_fea_pdf, tst_ore_tfidf_fea_pdf, \n trn_ore_tfidf_tgt_ser, tst_ore_tfidf_tgt_ser\n) = \\\ncreate_train_test_ts(fea_pdf = fea_tgt_ore_tfidf_pdf.drop( 'target',axis=1),\n                     tgt_ser = fea_tgt_ore_tfidf_pdf.loc[:,'target'],\n                    )\n\ntrn_ore_tfidf_fea_pdf.shape, tst_ore_tfidf_fea_pdf.shape, trn_ore_tfidf_tgt_ser.shape, tst_ore_tfidf_tgt_ser.shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">77</span><span class=\"ansired\">]: </span>((1273, 39616), (319, 39616), (1273,), (319,))\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["(trn_ore_cnt_fea_pdf, tst_ore_cnt_fea_pdf, \n trn_ore_cnt_tgt_ser, tst_ore_cnt_tgt_ser\n) = \\\ncreate_train_test_ts(fea_pdf = fea_tgt_ore_cnt_pdf.drop( 'target',axis=1),\n                     tgt_ser = fea_tgt_ore_cnt_pdf.loc[:,'target'],\n                    )\n\ntrn_ore_cnt_fea_pdf.shape, tst_ore_cnt_fea_pdf.shape, trn_ore_cnt_tgt_ser.shape, tst_ore_cnt_tgt_ser.shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">78</span><span class=\"ansired\">]: </span>((1273, 39616), (319, 39616), (1273,), (319,))\n</div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["## 3. Summary"],"metadata":{}},{"cell_type":"markdown","source":["Several preprocessing pipelines have been used to create the feature dataset and target dataset. Then we split the pre-processed dataframe into the train and test datasets with 80% used to train the models and 20% to test them. \n\nFour sets of train test datasets formed by the different combination of the merged datasets: \n - `bci_coal_pdf` with CountVectorizer \n - `bci_coal_pdf` with TfidfVectorizer\n - `bci_ironore_pdf` with CountVectorizer \n - `bci_ironore_pdf` with TfidfVectorizer\nThese combinations of train test dataset will be used in the estimator pipelines as well as in the gridsearch and compared with their scores."],"metadata":{}}],"metadata":{"name":"2. Preprocessing pipelines","notebookId":4127496387311794},"nbformat":4,"nbformat_minor":0}
