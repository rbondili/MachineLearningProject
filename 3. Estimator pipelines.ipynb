{"cells":[{"cell_type":"markdown","source":["#MA707 Report - Estimator Pipelines (spring 2019, DataHeroes)"],"metadata":{}},{"cell_type":"markdown","source":["## Introduction\nThis notebook contains code to create estimator pipelines which include the feature selection classes and the wrapped estimator classes. All the different train-test dataset defined in the `Preprocessing pipeline` will be fitted and tested to get their individual score at the default parameters as defined. \nThe code will be reused in the `Investigation` notebook where it will be fitted to various pipelines with different hyperparameters in GridSeach and get the `mean_test_scores` and their rank. \n\nThe train-test dataset used for fitting the models and predicting in this notebooks are:\n- [Dataframe 1: `fea_tgt_coal_tfidf_pdf` ](https://bentley.cloud.databricks.com/#notebook/1364451/command/1603805)\n- [Dataframe 2: `fea_tgt_coal_cnt_pdf`](https://bentley.cloud.databricks.com/#notebook/1364451/command/1603807)\n- [Dataframe 3: `fea_tgt_ore_tfidf_pdf`](https://bentley.cloud.databricks.com/#notebook/1364451/command/1603808)\n- [Dataframe 4: `fea_tgt_ore_cnt_pdf`](https://bentley.cloud.databricks.com/#notebook/1364451/command/1603809)"],"metadata":{}},{"cell_type":"markdown","source":["## Contents\n1. Setup"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Setup"],"metadata":{}},{"cell_type":"code","source":["%run \"./2. Preprocessing pipelines\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Lasso Pipelines"],"metadata":{}},{"cell_type":"markdown","source":["Least Absolute Shrinkage Selector Operator in short Lasso regression not only helps in reducing over-fitting but it can help in feature selection. It has the ability to shrink the estimated coefficient for the model to exactly zero, reducing the number of features and serve as a feature selection tool as well as the regression tool at the same time. By adding a biasing term to the regression optimization function to reduce the effect of collinearity. Lasso helps reduce the multicollinearity of endogenous variables in models which creates inaccurate estimates of the regression coefficients. \n\nThe [EstimatorLasso()](https://bentley.cloud.databricks.com/#notebook/1607089/command/1607095) wrapper class have the default parameters `alpha=1`  and `normalize=True` so the regressors X will be normalized before regression. \n \n**Note:** In order to get the best value of `alpha`, the data in the training set will be used to get estimate coefficients’ value for every value of `alpha`. In the validation set, different values of the product of `alpha` and coefficient estimates will be assessed. The one which has lower error value will get selected. This selected value of coefficients’ will again be assessed by test data set."],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_lasso_pipeline():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  from sklearn.linear_model import LinearRegression, Ridge\n  from sklearn.feature_selection import chi2\n  return Pipeline(steps=[\n    ('lso', EstimatorLasso())\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Fit the different train-test datasets into the above estimator pipeline and compare the scores."],"metadata":{}},{"cell_type":"code","source":["get_lasso_pipeline() \\\n  .fit  (trn_coal_tfidf_fea_pdf, trn_coal_tfidf_tgt_ser) \\\n  .score(tst_coal_tfidf_fea_pdf, tst_coal_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">210</span><span class=\"ansired\">]: </span>0.6996321665031258\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["get_lasso_pipeline() \\\n  .fit  (trn_coal_cnt_fea_pdf, trn_coal_cnt_tgt_ser) \\\n  .score(tst_coal_cnt_fea_pdf, tst_coal_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">211</span><span class=\"ansired\">]: </span>0.7221271878743873\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["get_lasso_pipeline() \\\n  .fit  (trn_ore_tfidf_fea_pdf, trn_ore_tfidf_tgt_ser) \\\n  .score(tst_ore_tfidf_fea_pdf, tst_ore_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">212</span><span class=\"ansired\">]: </span>0.7180596111631561\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["get_lasso_pipeline() \\\n  .fit  (trn_ore_cnt_fea_pdf, trn_ore_cnt_tgt_ser) \\\n  .score(tst_ore_cnt_fea_pdf, tst_ore_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">213</span><span class=\"ansired\">]: </span>0.6869873953834693\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["The Lasso regression is performed on the 3 day lag data of `bci` and 3 day lag data of counts and frequency of `tags`, `title` and `content` of the different datasets created in the pre-processing noteboook. The table below shows the results for each dataset. <table>\n    <tr>\n        <td>`bci_coal_pdf`</td><td>`bci_ironore_pdf`</td><td>`CountVectorizer`</td><td>`TfidfVectorizer`</td><td>`R Square`</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td></td><td>Yes</td><td></td><td>72.21%</td>\n    </tr>\n    <tr>\n        <td></td><td>Yes</td><td>Yes</td><td></td><td>68.69%</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td></td><td></td><td>Yes</td><td>69.96</td>\n    </tr>\n    <tr>\n        <td></td><td>Yes</td><td></td><td>Yes</td><td>71.80%</td>\n    </tr>  \n</table>\n\n\nFrom the results, it can be seen that the train-test dataset created by performing the feature extraction `CountVectorizer` method on the `bci_coal_pdf` and `TfidfVectorizer` on the `bci_ironore_pdf` both have the R Square (R2) value close to 72%.  R-square is the coefficient of determination which determines the proportion of the variability explained by the model. Later in the investigation notebook, these two datasets will try to run `GridSearchCV` to fine tune the hyper parameters and improve the model efficiency."],"metadata":{}},{"cell_type":"markdown","source":["##PCA Lasso Pipeline"],"metadata":{}},{"cell_type":"markdown","source":["Extending the Lasso model would be to add another feature reduction process to reduce the features and in process to check the model prediction.  \n\nAdd the [FeatureSelectionPCA](https://bentley.cloud.databricks.com/#notebook/1607074/command/1607080) wrapper class which has the base class of PCA (Principal component analysis). It is a technique for feature extraction — so it combines the input variables in a specific way, then can drop the “least important” variables while still retaining the most valuable parts of all of the variables. So here below creating a pipeline by using the `FeatureSelectionPCA()` wrapper class which have default parameters\n - `n_components=10` :- Means here it will create 10 new featuers using the existing features."],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_pca_lasso_pipeline():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  from sklearn.linear_model import LinearRegression, Ridge\n  return Pipeline(steps=[\n    ('pca', FeatureSelectionPCA()),\n    ('lso', EstimatorLasso())\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["Fit the estimator pipeline with the two classes and compare the scores with the different training and testing sets."],"metadata":{}},{"cell_type":"code","source":["get_pca_lasso_pipeline() \\\n  .fit  (trn_coal_tfidf_fea_pdf, trn_coal_tfidf_tgt_ser) \\\n  .score(tst_coal_tfidf_fea_pdf, tst_coal_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">215</span><span class=\"ansired\">]: </span>0.5705490311724806\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["get_pca_lasso_pipeline() \\\n  .fit  (trn_coal_cnt_fea_pdf, trn_coal_cnt_tgt_ser) \\\n  .score(tst_coal_cnt_fea_pdf, tst_coal_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">216</span><span class=\"ansired\">]: </span>0.5531753188892263\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["get_pca_lasso_pipeline() \\\n  .fit  (trn_ore_tfidf_fea_pdf, trn_ore_tfidf_tgt_ser) \\\n  .score(tst_ore_tfidf_fea_pdf, tst_ore_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">217</span><span class=\"ansired\">]: </span>0.5721601812600043\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["get_pca_lasso_pipeline() \\\n  .fit  (trn_ore_cnt_fea_pdf, trn_ore_cnt_tgt_ser) \\\n  .score(tst_ore_cnt_fea_pdf, tst_ore_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">218</span><span class=\"ansired\">]: </span>0.5521581673391068\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["%md PCA-Lasso pipeline performed on the below combination of datasets, the table below shows the results.  \n <table>\n    <tr>\n        <td>`bci_coal_pdf`</td><td>`bci_ironore_pdf`</td><td>`CountVectorizer`</td><td>`TfidfVectorizer`</td><td>`R Square`</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td></td><td>Yes</td><td></td><td>55.31%</td>\n    </tr>\n    <tr>\n        <td></td><td>Yes</td><td>Yes</td><td></td><td>55.21%</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td></td><td></td><td>Yes</td><td>57.05%</td>\n    </tr>\n    <tr>\n        <td></td><td>Yes</td><td></td><td>Yes</td><td>57.21%</td>\n    </tr>  \n</table>\n\nAs we see `bci_coal_pdf with CountVectorizer` and `bci_ironore_pdf with TfidfVectorizer` both have the R Square (R2) value of around 55% to 57%. Eventhough there is a drop in the variability with the addition of PCA, but as default only 10 features have been selected as default. This estimator pipeline will be tried  using `GridSearchCV` with addtion of many features by fine tuning the hyper parameters to check any improvement in the model efficiency."],"metadata":{}},{"cell_type":"markdown","source":["### ElasticNet Pipelines"],"metadata":{}},{"cell_type":"markdown","source":["ElasticNet is a third commonly used model of regression which incorporates penalties from both Ridge and Lasso models. Ridge regression model is explained in the following sections. Addition to setting and choosing a lambda value elastic net also allows to tune the alpha parameter where 𝞪 = 0 corresponds to ridge and 𝞪 = 1 to lasso. Therefore an alpha value can be chosen between 0 and 1 to optimize the elastic net. ElasticNet keeps the group effect in the case of highly correlated variables, rather than zeroing some of them like Lasso and also no limitation on the number of selected variables. \n\nThe [EstimatorElasticNet()](https://bentley.cloud.databricks.com/#notebook/1607089/command/1607099) wrapper class which have default parameters `alpha=0.5`it gives the equal weight to both Ridge and Lasso lambda values and `normalize=True` which regressors X will be normalized before regression. \n \n **Note:** In order to get the best value of `alpha`, the data in the training set will be used to get estimate coefficients’ value for every value of `alpha`. In the validation set, different values of the product of `alpha` and coefficient estimates will be assessed. The one which has lower error value will get selected. This selected value of coefficients’ will again be assessed by test data set."],"metadata":{}},{"cell_type":"markdown","source":["ElasticNet regression is performed on the below combination of datasets."],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_elasticnet_pipeline():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  from sklearn.linear_model import LinearRegression, Ridge\n  return Pipeline(steps=[\n    ('ela', EstimatorElasticNet())\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["get_elasticnet_pipeline() \\\n  .fit  (trn_coal_tfidf_fea_pdf, trn_coal_tfidf_tgt_ser) \\\n  .score(tst_coal_tfidf_fea_pdf, tst_coal_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">220</span><span class=\"ansired\">]: </span>-1.6026201763340326\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["get_elasticnet_pipeline() \\\n  .fit  (trn_coal_cnt_fea_pdf, trn_coal_cnt_tgt_ser) \\\n  .score(tst_coal_cnt_fea_pdf, tst_coal_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">221</span><span class=\"ansired\">]: </span>-1.595733877104839\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["get_elasticnet_pipeline() \\\n  .fit  (trn_ore_tfidf_fea_pdf, trn_ore_tfidf_tgt_ser) \\\n  .score(tst_ore_tfidf_fea_pdf, tst_ore_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">222</span><span class=\"ansired\">]: </span>-1.5649748609984355\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["get_elasticnet_pipeline() \\\n  .fit  (trn_ore_cnt_fea_pdf, trn_ore_cnt_tgt_ser) \\\n  .score(tst_ore_cnt_fea_pdf, tst_ore_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">223</span><span class=\"ansired\">]: </span>-1.5663365684287252\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["d %md The table below shows the results (r-square value) for the different datasets fitted into the Elaastic net estimator pipeline.\n  <table>\n    <tr>\n        <td>`bci_coal_pdf`</td><td>`bci_ironore_pdf`</td><td>`CountVectorizer`</td><td>`TfidfVectorizer`</td><td>`R Square`</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td></td><td>Yes</td><td></td><td> -1.595</td>\n    </tr>\n    <tr>\n        <td></td><td>Yes</td><td>Yes</td><td></td><td>-1.566</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td></td><td></td><td>Yes</td><td>-1.603</td>\n    </tr>\n    <tr>\n        <td></td><td>Yes</td><td></td><td>Yes</td><td>-1.565</td>\n    </tr>  \n</table>\n\nAs we see for all the combinations have the R Square (R2) the proportion of the variability explained by the models is negative which means the model is not good. Will try to run `GridSearchCV` to check if any scope of improvement, also like to check the same by adding PCA too. Since all of the test scores are negative, we try Elastic Net model with PCA to improve the model performance\n\nExtending the Elastic model by adding another feature reduction process PCA to check if the overall predicton will improve."],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_pca_elasticnet_pipeline():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  from sklearn.linear_model import LinearRegression, Ridge\n  return Pipeline(steps=[\n    ('pca', FeatureSelectionPCA()),\n    ('ela', EstimatorElasticNet())\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["get_pca_elasticnet_pipeline() \\\n  .fit  (trn_coal_tfidf_fea_pdf, trn_coal_tfidf_tgt_ser) \\\n  .score(tst_coal_tfidf_fea_pdf, tst_coal_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">225</span><span class=\"ansired\">]: </span>-1.652031389105387\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["get_pca_elasticnet_pipeline() \\\n  .fit  (trn_coal_cnt_fea_pdf, trn_coal_cnt_tgt_ser) \\\n  .score(tst_coal_cnt_fea_pdf, tst_coal_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">226</span><span class=\"ansired\">]: </span>-1.6512141723974794\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["get_pca_elasticnet_pipeline() \\\n  .fit  (trn_ore_tfidf_fea_pdf, trn_ore_tfidf_tgt_ser) \\\n  .score(tst_ore_tfidf_fea_pdf, tst_ore_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">227</span><span class=\"ansired\">]: </span>-1.641098898459107\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["get_pca_elasticnet_pipeline() \\\n  .fit  (trn_ore_cnt_fea_pdf, trn_ore_cnt_tgt_ser) \\\n  .score(tst_ore_cnt_fea_pdf, tst_ore_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">228</span><span class=\"ansired\">]: </span>-1.6402270398874297\n</div>"]}}],"execution_count":37},{"cell_type":"markdown","source":["ElasticNet regression with PCA performed on the above combination of datasets. As noticed ealier the  R Square of the models is still negative which means the model is not good."],"metadata":{}},{"cell_type":"markdown","source":["### SVR Pipelines"],"metadata":{}},{"cell_type":"markdown","source":["Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences. In simple regression we try to minimise the error rate. While in SVR we try to fit the error within a certain threshold.  Objective with SVR is to basically consider the points that are within the boundary line, so best fit line is the line hyperplane that has maximum number of points."],"metadata":{}},{"cell_type":"markdown","source":["Build the `get_svr_pipeline` function which returns a pipeline [Estimatorsvr](https://bentley.cloud.databricks.com/#notebook/1607089/command/1607103) which calls the base class `SVR` with default parameters `kernel='rbf', gamma='auto', C=100`. Call the function and fit the pipeline with\n4 different datasets then get the R score on the relative test datasets. Compare the scores."],"metadata":{}},{"cell_type":"code","source":["%python \ndef get_svr_pipeline():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  from sklearn.svm               import SVR\n  from sklearn.feature_selection import chi2\n  return Pipeline(steps=[\n    ('svr', EstimatorSVR())\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"code","source":["get_svr_pipeline() \\\n  .fit  (trn_coal_tfidf_fea_pdf, trn_coal_tfidf_tgt_ser) \\\n  .score(tst_coal_tfidf_fea_pdf, tst_coal_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">230</span><span class=\"ansired\">]: </span>-2.8959249004785232\n</div>"]}}],"execution_count":43},{"cell_type":"code","source":["get_svr_pipeline() \\\n  .fit  (trn_coal_cnt_fea_pdf, trn_coal_cnt_tgt_ser) \\\n  .score(tst_coal_cnt_fea_pdf, tst_coal_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">231</span><span class=\"ansired\">]: </span>-2.89630577106593\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["get_svr_pipeline() \\\n  .fit  (trn_ore_tfidf_fea_pdf, trn_ore_tfidf_tgt_ser) \\\n  .score(tst_ore_tfidf_fea_pdf, tst_ore_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">232</span><span class=\"ansired\">]: </span>-2.8811050016842903\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["get_svr_pipeline() \\\n  .fit  (trn_ore_cnt_fea_pdf, trn_ore_cnt_tgt_ser) \\\n  .score(tst_ore_cnt_fea_pdf, tst_ore_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">233</span><span class=\"ansired\">]: </span>-2.8810047965738446\n</div>"]}}],"execution_count":46},{"cell_type":"markdown","source":["The scores are as following: -2.896, -2.896, -2.881, -2.881. The scores are all \nnegative for the default parameters in the model, the model appears to be not working very well with the datasets. SVR regression also results in a R Square (R2) value which is negative so the model is not good to be used. So inorder to get better results from this model, it will be tested using the GridSearchCV with different parameters other than the default values during investigation."],"metadata":{}},{"cell_type":"markdown","source":["###Ridge Pipelines"],"metadata":{}},{"cell_type":"markdown","source":["Ridge regression is a regression technique for continuous value prediction. It is a simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression. Using a ridge regression method helps reduce the multicollinearity of endogenous variables in models. Multicollinearity creates inaccurate estimates of the regression coefficients.\n\nGenerally when overfitting happens, the regression coefficients’ values becomes very huge. Ridge regression is used to quantify the overfitting of the data through measuring the magnitude of coefficients.  The parameter `alpha` as defined in the wrapper class `EstimatorRidge` in *Notebook 0.4* is the tuning parameter to balance the fit of data and magnitude of coefficients.\n\nThe class [EstimatorRidge()](https://bentley.cloud.databricks.com/#notebook/1607089/command/1607091) which calls on the base class `Ridge` from sklearn.linear_model package, with all its default parameter `normalize=False` and `solver='auto'` which chooses the solver automatically based on the type of data.\n\n**Note:** Inorder to get the best value of `alpha`, the data in the training set will be used to get estimate coefficients’ value for every value of `alpha`. In the validation set, different values of the product of `alpha` and coefficient estimates will be assessed. The one which has lower error value will get selected. This selected value of coefficients’ will again be assessed by test data set."],"metadata":{}},{"cell_type":"markdown","source":["Build `get_ridge_pipeline` function, it returns a pipeline with the estimator class [estimatorRidge](https://bentley.cloud.databricks.com/#notebook/1607089/command/1607091). Call the function and fit the pipeline with\n4 different datasets then get the R score on the relative test datasets."],"metadata":{}},{"cell_type":"code","source":["def get_ridge_pipeline():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  from sklearn.linear_model import LinearRegression, Ridge\n  return Pipeline(steps=[\n    ('rdg', EstimatorRidge())\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"code","source":["get_ridge_pipeline() \\\n  .fit  (trn_coal_tfidf_fea_pdf, trn_coal_tfidf_tgt_ser) \\\n  .score(tst_coal_tfidf_fea_pdf, tst_coal_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">236</span><span class=\"ansired\">]: </span>-0.971180299717989\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":["get_ridge_pipeline() \\\n  .fit  (trn_coal_cnt_fea_pdf, trn_coal_cnt_tgt_ser) \\\n  .score(tst_coal_cnt_fea_pdf, tst_coal_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">237</span><span class=\"ansired\">]: </span>-0.8353216169750179\n</div>"]}}],"execution_count":53},{"cell_type":"code","source":["get_ridge_pipeline() \\\n  .fit  (trn_ore_tfidf_fea_pdf, trn_ore_tfidf_tgt_ser) \\\n  .score(tst_ore_tfidf_fea_pdf, tst_ore_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">238</span><span class=\"ansired\">]: </span>-0.7446500148891255\n</div>"]}}],"execution_count":54},{"cell_type":"code","source":["get_ridge_pipeline() \\\n  .fit  (trn_ore_cnt_fea_pdf, trn_ore_cnt_tgt_ser) \\\n  .score(tst_ore_cnt_fea_pdf, tst_ore_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">239</span><span class=\"ansired\">]: </span>-0.5491851344825782\n</div>"]}}],"execution_count":55},{"cell_type":"markdown","source":["The scores are as following: -0.971, -0.835, -0.744, -0.549. The scores are all \nnegative with the default parameters for the wrapper class. Hence in the Investigation notebook different hyperparameters will be tested to fine tune the model during gridsearch and results will be compared and ranked based on their `mean_test_score` value."],"metadata":{}},{"cell_type":"markdown","source":["### Decision Tree"],"metadata":{}},{"cell_type":"markdown","source":["A decision tree is a largely used non-parametric effective machine learning modeling technique for regression and classification problems. To find solutions a decision tree makes sequential, hierarchical decision about the outcomes variable based on the predictor data.\n\nHierarchical means the model is defined by a series of questions that lead to a class label or a value when applied to any observation. Once set up the model acts like a protocol in a series of “if this occurs then this occurs” conditions that produce a specific result from the input data.\n\nA Non-parametric method means that there are no underlying assumptions about the distribution of the errors or the data. It basically means that the model is constructed based on the observed data."],"metadata":{}},{"cell_type":"markdown","source":["Build the `get_DecisionTree_pipeline` function which returns a pipeline with an estimator [EstimatorDecisiontree](https://bentley.cloud.databricks.com/#notebook/1607089/command/1607111) which has a base class of `DecisionTreeRegressor` from `sklearn.tree`. The default parameter for the estimator is set as `max_depth=5, max_leaf_nodes=10`. Call the function and fit the pipeline with\n4 different datasets then get the R score on the relative test datasets."],"metadata":{}},{"cell_type":"code","source":["def get_DecisionTree_pipeline():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  from sklearn.tree     import DecisionTreeRegressor\n  return Pipeline(steps=[\n    ('dtr',EstimatorDecisiontree())\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":60},{"cell_type":"code","source":["get_DecisionTree_pipeline() \\\n  .fit  (trn_coal_tfidf_fea_pdf, trn_coal_tfidf_tgt_ser) \\\n  .score(tst_coal_tfidf_fea_pdf, tst_coal_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">241</span><span class=\"ansired\">]: </span>0.12304391486498313\n</div>"]}}],"execution_count":61},{"cell_type":"code","source":["get_DecisionTree_pipeline() \\\n  .fit  (trn_coal_cnt_fea_pdf, trn_coal_cnt_tgt_ser) \\\n  .score(tst_coal_cnt_fea_pdf, tst_coal_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">242</span><span class=\"ansired\">]: </span>0.24612626349563116\n</div>"]}}],"execution_count":62},{"cell_type":"code","source":["get_DecisionTree_pipeline() \\\n  .fit  (trn_ore_tfidf_fea_pdf, trn_ore_tfidf_tgt_ser) \\\n  .score(tst_ore_tfidf_fea_pdf, tst_ore_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">243</span><span class=\"ansired\">]: </span>0.08139281914497176\n</div>"]}}],"execution_count":63},{"cell_type":"code","source":["get_DecisionTree_pipeline() \\\n  .fit  (trn_ore_cnt_fea_pdf, trn_ore_cnt_tgt_ser) \\\n  .score(tst_ore_cnt_fea_pdf, tst_ore_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">244</span><span class=\"ansired\">]: </span>0.10022421639724532\n</div>"]}}],"execution_count":64},{"cell_type":"markdown","source":["The scores are as following: 0.123, 0.246, 0.081, 0.100. It can be concluded that\nthe decision tree works best with `bci_coal_pdf` dataset containing features extracted using the CountVectorizer method."],"metadata":{}},{"cell_type":"markdown","source":["### Random Forest"],"metadata":{}},{"cell_type":"markdown","source":["Random forests, also known as random decision forests, are a popular ensemble method that can be used to build predictive models for both classification and regression problems. Ensemble methods use multiple learning models to gain better predictive results — in the case of a random forest, the model creates an entire forest of random uncorrelated decision trees to arrive at the best possible answer."],"metadata":{}},{"cell_type":"code","source":["def get_RandomForest_pipeline():\n  from sklearn.pipeline import FeatureUnion, Pipeline\n  from sklearn.tree     import DecisionTreeRegressor\n  return Pipeline(steps=[\n    ('rf',EstimatorRandomForest())\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":68},{"cell_type":"markdown","source":["Build `get_RandomForest_pipeline` function which returns a pipeline with the estimator [EstimatorRandomForest](https://bentley.cloud.databricks.com/#notebook/1607089/command/1607114), which has the base class `RandomForestRegressor` with the default parameters `n_estimators=5`, and `max_leaf_nodes=10`. Call the function and fit the pipeline with\n4 different datasets then get the R score on the relative test datasets."],"metadata":{}},{"cell_type":"code","source":["get_RandomForest_pipeline() \\\n  .fit  (trn_coal_tfidf_fea_pdf, trn_coal_tfidf_tgt_ser) \\\n  .score(tst_coal_tfidf_fea_pdf, tst_coal_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">246</span><span class=\"ansired\">]: </span>0.566372283449996\n</div>"]}}],"execution_count":70},{"cell_type":"code","source":["get_RandomForest_pipeline() \\\n  .fit  (trn_coal_cnt_fea_pdf, trn_coal_cnt_tgt_ser) \\\n  .score(tst_coal_cnt_fea_pdf, tst_coal_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">247</span><span class=\"ansired\">]: </span>0.6397947673873587\n</div>"]}}],"execution_count":71},{"cell_type":"code","source":["get_RandomForest_pipeline() \\\n  .fit  (trn_ore_tfidf_fea_pdf, trn_ore_tfidf_tgt_ser) \\\n  .score(tst_ore_tfidf_fea_pdf, tst_ore_tfidf_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">248</span><span class=\"ansired\">]: </span>0.6528499130954133\n</div>"]}}],"execution_count":72},{"cell_type":"code","source":["get_RandomForest_pipeline() \\\n  .fit  (trn_ore_cnt_fea_pdf, trn_ore_cnt_tgt_ser) \\\n  .score(tst_ore_cnt_fea_pdf, tst_ore_cnt_tgt_ser)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">249</span><span class=\"ansired\">]: </span>0.5787878651636065\n</div>"]}}],"execution_count":73},{"cell_type":"markdown","source":["The scores are as following: 0.566, 0.639, 0.653, 0.579. It can be concluded that the decision tree works best with the dataset created from the merged `bci_ironore_pdf` features and `tfidfVectorizer` features."],"metadata":{}},{"cell_type":"markdown","source":["## Summary"],"metadata":{}},{"cell_type":"markdown","source":["In this notebook different estimator pipelines were built and tested with their default parameters on the four different datasets created in Preprocessing pipeline.\nHere's the score rank of all 7 models with the best outcome:\n\n- Lasso Regression: 0.722 \n- Random Forest: 0.691\n- Lasso with PCA: 0.572\n- Decision Tree: 0.246\n- Ridge Regression: -0.547\n- Elastic Net: - 1.602\n- SVR: -2.881"],"metadata":{}},{"cell_type":"markdown","source":["The best Pipeline with a high `mean_test_score` of the R-square values is **Lasso Regression** with default parameters of `alpha=1` and `normalize='True'`. The scores of the model fitted and predicted on the train test dataset created from `bci_coal_pdf` dataset with features created using the `CounVectorizer` method and the `bci_ironore_pdf` dataset with additional features created using the `TfidfVectorizer` method are as below. <table>\n    <tr>\n        <td>`bci_coal_pdf`</td><td>`bci_ironore_pdf`</td><td>`CountVectorizer`</td><td>`TfidfVectorizer`</td><td>`R Square`</td>\n    </tr>\n    <tr>\n        <td>Yes</td><td></td><td>Yes</td><td></td><td>72.21%</td>\n    </tr>\n    <tr>\n    <tr>\n        <td></td><td>Yes</td><td></td><td>Yes</td><td>71.80%</td>\n    </tr>  \n</table>\nHence in the Investigation models where gridsearch will be performed with different hyperparameters, these two combination of train-test datasets will be used to train and model and get the best score of each model and then select the best model with its hyperparameters."],"metadata":{}}],"metadata":{"name":"3. Estimator pipelines","notebookId":4127496387311547},"nbformat":4,"nbformat_minor":0}
